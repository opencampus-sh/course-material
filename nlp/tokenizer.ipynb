{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for Tokenizations\n",
    "### Without Chat Template\n",
    "For example, as used in pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 50.6k/50.6k [00:00<00:00, 484kB/s]\n",
      "tokenizer.json: 100%|██████████| 9.09M/9.09M [00:01<00:00, 5.08MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 73.0/73.0 [00:00<00:00, 9.67kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  [[128000, 33883, 27439, 24578, 2503, 28311, 13], [128000, 54850, 80292, 1342, 13]]\n",
      "Tokens:  ['<|begin_of_text|>', 'Lorem', 'Ġipsum', 'Ġdolor', 'Ġsit', 'Ġamet', '.']\n",
      "Tokens:  ['<|begin_of_text|>', 'Ein', 'ĠBeispiel', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "HF_API_KEY = os.getenv('HF_API_KEY')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=HF_API_KEY)\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\"Lorem ipsum dolor sit amet.\",\"Ein Beispieltext.\"]\n",
    "# Apply the tokenizer to the messages\n",
    "tokenized_prompt = tokenizer(messages)\n",
    "\n",
    "# Print the tokenized inputs\n",
    "print(\"Token IDs: \", tokenized_prompt['input_ids'])\n",
    "print(\"Tokens: \", tokenizer.convert_ids_to_tokens(tokenized_prompt['input_ids'][0]))\n",
    "print(\"Tokens: \", tokenizer.convert_ids_to_tokens(tokenized_prompt['input_ids'][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Chat Template\n",
    "As used in instruction tuning for chat bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  tensor([[128000,     27,     91,    318,   5011,     91,     29,   9125,    198,\n",
      "           2675,    527,    264,  11919,   6369,   6465,    889,   2744,  31680,\n",
      "            304,    279,   1742,    315,    264,  55066,     27,     91,    318,\n",
      "           6345,     91,    397,     27,     91,    318,   5011,     91,     29,\n",
      "            882,    198,   4438,   1690,  59432,    649,    264,   3823,   8343,\n",
      "            304,    832,  11961,  76514,     91,    318,   6345,     91,    397,\n",
      "             27,     91,    318,   5011,     91,     29,  78191,    198]])\n",
      "Tokens:  ['<|begin_of_text|>', '<', '|', 'im', '_start', '|', '>', 'system', 'Ċ', 'You', 'Ġare', 'Ġa', 'Ġfriendly', 'Ġchat', 'bot', 'Ġwho', 'Ġalways', 'Ġresponds', 'Ġin', 'Ġthe', 'Ġstyle', 'Ġof', 'Ġa', 'Ġpirate', '<', '|', 'im', '_end', '|', '>Ċ', '<', '|', 'im', '_start', '|', '>', 'user', 'Ċ', 'How', 'Ġmany', 'Ġhelicopters', 'Ġcan', 'Ġa', 'Ġhuman', 'Ġeat', 'Ġin', 'Ġone', 'Ġsitting', '?<', '|', 'im', '_end', '|', '>Ċ', '<', '|', 'im', '_start', '|', '>', 'assistant', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=HF_API_KEY)\n",
    "\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "# Apply the chat template but do not tokenize the result yet\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Now, tokenize the prompt to see how it looks in its tokenized form\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(\"Token IDs: \", tokenized_prompt['input_ids'])\n",
    "print(\"Tokens: \", tokenizer.convert_ids_to_tokens(tokenized_prompt['input_ids'][0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
