{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yoda Language Model Training\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Import the yoda-corpus dataset.\n",
    "2. Train a tokenizer.\n",
    "3. Generate tokenized n-grams for each line in the corpus.\n",
    "4. Split the data into features and labels.\n",
    "5. Add padding to the sequences.\n",
    "6. Construct and train a neural network model.\n",
    "7. Implement a simple form of top-k sampling.\n",
    "8. Train a model with masked padding tokens.\n",
    "9. Test different models' performances.\n",
    "\n",
    "## Introduction\n",
    "In this exercise, we'll be using a corpus of sentences styled after the unique manner of speaking of the Star Wars character Master Yoda.  \n",
    "\n",
    "The dataset was generated using the large language model [Claude](https://claude.ai/). The initial prompt was:  \n",
    "\"Create 10 short sentences that mimic the style of Yoda.\"  \n",
    "\n",
    "Via the prompt \"Generate 200 more short sentences.\" were then several batches of 200 examples generated.\n",
    "In a further step examples that included full stops, question marks, and exclamation marks were split into seperate examples (some \"short\" sentences were actually not that short and included more than one sentence). Finally, the examples were distilled by using fuzzy comparison, and for all examples that had a similarity of more than 95% only one example was kept.\n",
    "\n",
    "You can download the `yoda-corpus.txt` file [here](https://github.com/opencampus-sh/course-material/blob/main/machine-learning-with-tensorflow/week-06/yoda-corpus.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Yoda-Corpus Dataset\n",
    "Before proceeding, ensure that you have downloaded the `yoda-corpus.txt` and uploaded it to your Google Drive in a specified folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount your Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a pandas dataframe.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file as a plain text file since the file is not formatted as csv\n",
    "with open(\"/content/drive/MyDrive/path_to_your_file\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\n",
    "    \"Average number of words in each text: \",\n",
    "    df[\"text\"].apply(lambda x: len(x.split(\" \"))).mean(),\n",
    ")\n",
    "print(\n",
    "    \"Shortest text: \",\n",
    "    df[\"text\"].apply(lambda x: len(x.split(\" \"))).min(),\n",
    ")\n",
    "print(\n",
    "    \"Longest text: \",\n",
    "    df[\"text\"].apply(lambda x: len(x.split(\" \"))).max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "First, we define and add special tokens to our training data, then we train the Tokenizer on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# at the end of each sentence add end-of-sequence token \"<EOS>\"\n",
    "df[\"text\"] = df[\"text\"] + \"<EOS>\"\n",
    "\n",
    "# train tokenizer on text\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "\n",
    "# get vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print vocabulary size\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the tokenizer to convert the texts into sequences of integers. Additionally, the following code creates all possible n-grams from the tokenized sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all n-grams from a single sentence (row)\n",
    "def create_ngrams_from_sentence(tokenized_text):\n",
    "    sentence_ngrams = []\n",
    "    for i in range(1, len(tokenized_text)):\n",
    "        sentence_ngrams.append(tokenized_text[: i + 1])\n",
    "    return sentence_ngrams\n",
    "\n",
    "\n",
    "# collect all n-grams from all sentences of the corpus in a single list\n",
    "corpus_ngrams = []\n",
    "for row in df[\"text\"]:\n",
    "    tokenized_text = tokenizer.texts_to_sequences([row])[0]\n",
    "    corpus_ngrams.extend(create_ngrams_from_sentence(tokenized_text))\n",
    "\n",
    "# print the first 20 n-grams\n",
    "corpus_ngrams[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Features and Labels\n",
    "The last element of each sequence will be our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = [(i[: len(i) - 1]) for i in corpus_ngrams]\n",
    "labels = [i[len(i) - 1] for i in corpus_ngrams]\n",
    "\n",
    "# print the first 10 features and labels\n",
    "print(\"Features: \", features[:10])\n",
    "print(\"Labels: \", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature sequences need to be padded to ensure they have the same length and turn features and labels into a numpy array as needed for Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = XXX  # Replace with the maximum sequence length your model will be trained with (context window size)\n",
    "numpy_features = np.array(pad_sequences(features, padding=\"post\", maxlen=max_len))\n",
    "numpy_labels = np.array(labels)\n",
    "# print the first 10 padded features and labels in numpy format\n",
    "print(\"Features: \", numpy_features[:10])\n",
    "print(\"Labels: \", numpy_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction and Training\n",
    "\n",
    "Insert the correct value for the input dimension of the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Model parameters\n",
    "input_dim = XXX  # Replace with the correct value\n",
    "output_dim = 128  # the dimensionality of the embedding vectors\n",
    "input_length = max_len  # the maximum sequence length your model will be trained with (context window size)\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(input_dim, activation=\"softmax\"))  # Output layer\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(numpy_features, numpy_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the loss and accuracy curves to review the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "Let's see our model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_yoda_speak(seed_text, max_words):\n",
    "    for _ in range(max_words):\n",
    "        # Tokenize the sentence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Pad the sentence to the maximum length\n",
    "        token_list = pad_sequences([token_list], maxlen=max_len, padding=\"post\")\n",
    "        # Get the predicted probabilities for each word\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        # Get the index of the most probable word\n",
    "        predicted = np.argmax(predicted_probs, axis=-1).item()\n",
    "        # Convert index to word\n",
    "        output_word = tokenizer.index_word[predicted]\n",
    "        # Check if end of sequence token was generated\n",
    "        if output_word == \"eos\":\n",
    "            # terminate generation\n",
    "            break\n",
    "        # Add the predicted word to the seed text\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "\n",
    "prompt = \"Seek\"\n",
    "max_tokens = 15\n",
    "\n",
    "# Example usage\n",
    "print(generate_yoda_speak(prompt, max_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including Randomness in the Prediction (Top-K Sampling)  \n",
    "\n",
    "The above implemented inference function follows a so called \"greedy\" approach, that is, the predicted next word is the one with highest probability. In praxis, however, models often have a better output if you include randomness in the selection of the predicted word.\n",
    "\n",
    "Implement an inference function that randomly selects the predicted word from the 3 most probable words and compare the results with the ones from above.\n",
    "Vary the group size and compare the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_yoda_speak(prompt, max_words, k):\n",
    "    # INCLUDE YOUR CODE HERE\n",
    "    # (Hint: You can use the generate_yoda_speak function from above and just adjust one line of code)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "\n",
    "prompt = \"Seek\"\n",
    "max_tokens = 15\n",
    "k = 3\n",
    "\n",
    "print(generate_yoda_speak(prompt, max_tokens, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Padding Tokens\n",
    "\n",
    "Take the model definition from above abd add the argument `mask_zeros=true` to the definition of the embedding layer.  \n",
    "Train the model. Do you notice a difference?  \n",
    "\n",
    "Find out what is changed during training when `mask_zeros` is set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Define model architecture\n",
    "# INCLUDE YOUR CODE HERE\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(numpy_features, numpy_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the loss and accuracy curves to review the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
