{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi9KCIndN2A0"
      },
      "source": [
        "# Sentiment Analysis Using the Sentiment140 Dataset\n",
        "\n",
        "In this notebook, we aim to accomplish the following:\n",
        "1. Import and analyze the Sentiment140 dataset.\n",
        "2. Prepare the data for model training\n",
        "3. Implement a train-validation split to evaluate our model's performance.\n",
        "4. Train a tokenizer on the training data and tokenize the data\n",
        "5. Pad the data on a selected length\n",
        "6. Construct and train a neural network model that includes an embedding layer for text representation.\n",
        "7. Inspect the output dimensionality of the embedding layer.\n",
        "8. Train the model to classify the sentiment of tweets and evaluate its performance using the validation set.\n",
        "9. Tune the hyper parameters for the tokenization and model training.\n",
        "\n",
        "## Introduction\n",
        "For this exercise, we'll be using a subsample of the Sentiment140 dataset from Kaggle.\n",
        "Sentiment140 is a popular dataset for sentiment analysis that contains 1.6 million tweets labeled for sentiment. This dataset is widely used for training machine learning models to differentiate between positive and negative sentiment in text.\n",
        "\n",
        "You can download the dataset [here](https://github.com/opencampus-sh/course-material/blob/main/machine-learning-with-tensorflow/week-05/sentiment140_small.csv).\n",
        "\n",
        "Please download the dataset and upload it to your Google Drive.\n",
        "By uploading the data to your Google Drive and not directly to your Colab environment, it is persistently available. You just need to run the cell the code to mount your Google Drive to your Colab environment.\n",
        "\n",
        "The dataset contains the following 6 fields:  \n",
        "`target`: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)  \n",
        "`ids`: The id of the tweet ( 2087)  \n",
        "`date`: the date of the tweet (Sat May 16 23:58:44 UTC 2009)  \n",
        "`flag`: The query (lyx). If there is no query, then this value is NO_QUERY.  \n",
        "`user`: the user that tweeted (robotickilldozr)  \n",
        "`text`: the text of the tweet (Lyx is cool)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmpWkR72N2A-"
      },
      "source": [
        "## Google Drive Setup\n",
        "Before proceeding, ensure that you have uploaded the Sentiment140 dataset to your Google Drive in a specified folder. Then, use the following code to mount your Google Drive and access files using the path '/content/drive/MyDrive/'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jWwiDCON2A_",
        "outputId": "9b8f301b-c44c-432e-c4a0-4305e76e509d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0GBuvowN2BC"
      },
      "source": [
        "## Importing Data\n",
        "Next, we will import the dataset and examine it using descriptive statistics to gain initial insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxHn4UwnN2BD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define column names\n",
        "column_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# Update this to the path of your Sentiment140 dataset file\n",
        "file_path = '/content/drive/MyDrive/path_to_your_file.csv'\n",
        "data = pd.read_csv(file_path, encoding='ISO-8859-1', names=column_names)  # Note: The encoding may vary based on your dataset specifics\n",
        "\n",
        "data.head()  # View the first few rows of the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tni_ZsOyjPz2"
      },
      "source": [
        "### Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGnYyIIXjMwb",
        "outputId": "adf8a9da-376a-43ec-a041-b5a67c53d46a"
      },
      "outputs": [],
      "source": [
        "# Average number of words in each tweet\n",
        "data['text'].apply(lambda x: len(x.split(' '))).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "1Ne0LBjNO9Ui",
        "outputId": "c56edb67-8c5d-4bc4-dd5f-47b095c6ac02"
      },
      "outputs": [],
      "source": [
        "# Summary statistics for numerical columns\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MccXmBVrjeOY",
        "outputId": "49c74e60-2f9a-42b1-d0ff-e68d6ab89de7"
      },
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "data['your_label_column'].value_counts()  # Replace 'your_label_column' with the actual column name for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Labels must start at 0 and increase sequentially by 1 to mark the different classes\n",
        "data['label'] = # INCLUDE YOUR CODE HERE\n",
        "\n",
        "data['label'].value_counts() \n",
        "\n",
        "# Another option would be to use one-hot encoding to represent the labels, where each class is represented by a vector of 0s and a 1 in the position of the class label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lRiboucddpS"
      },
      "source": [
        "## Removing Stop Words\n",
        "\n",
        "Stop words are common words that generally do not contribute much meaning in a sentence and are typically removed in the preprocessing stage of traditional text analysis. This helps in reducing the size of the dataset and improves the performance of the model by focusing on words that carry more meaning.\n",
        "\n",
        "In this section, we will use the NLTK library, a widely-used Python library for natural language processing, to remove stop words from our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-MAaRXoddpT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stop_words(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [\" \".join([word for word in sentence.split() if word.lower() not in stop_words]) for sentence in data]\n",
        "\n",
        "# Add new column including the texts without stopwords\n",
        "data['text_without_stopwords'] = remove_stop_words(data['your_text_column'])\n",
        "\n",
        "# print the first few rows of the text columns\n",
        "data[['your_text_column', 'text_without_stopwords']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luOAFpHFi-VB",
        "outputId": "fdf725a2-0bd3-473f-96e5-c39444f38f7e"
      },
      "outputs": [],
      "source": [
        "# Average number of words in each tweet without stopwords\n",
        "data['text_without_stopwords'].apply(lambda x: len(x.split(' '))).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSv8UbdAmNJA"
      },
      "source": [
        "## Splitting Data into Training and Test Sets\n",
        "\n",
        "Before training our model, it is essential to split the dataset into training and test sets. This approach helps in assessing the performance of the model on unseen data, ensuring that our evaluations are realistic and our model is not overfitting to the training data.\n",
        "\n",
        "We will use a typical split ratio of 80% for training and 20% for testing. You can adjust this ratio based on your dataset size and requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L71P1g9mRsj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'data' is your dataframe and 'labels' is the column with sentiment labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['your_text'], data['your_labels'], test_size=0.2, random_state=42) # replace 'your_text' and 'your_labels' with the actual column names for text and labels\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQib5xbAN2BD"
      },
      "source": [
        "## Tokenization Function\n",
        "The following function will be responsible for tokenizing our text data. We'll use Keras' Tokenizer class, which allows us to vectorize a text corpus by turning each text into a sequence of integers. The `oov_token` parameter is used to handle out-of-vocabulary words during text conversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ra3iY7xlORg"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow library for text processing (if not already installed)\n",
        "%pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PGI4BarN2BE"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Implement tokenizer function\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\", num_words = 0) # Change the num_words parameter to the desired number of words that your dictionary should contain\n",
        "\n",
        "# Train the tokenizer\n",
        "# INCLUDE YOUR CODE HERE\n",
        "\n",
        "# Tokenize the data\n",
        "X_train_sequences = # INCLUDE YOUR CODE HERE\n",
        "\n",
        "# Create a datframe with the original and the tokenized data to check the tokenization for the first rows\n",
        "pd.DataFrame(zip(X_train, X_train_sequences), columns=['text', 'sequence']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y65J-BbkN2BG"
      },
      "source": [
        "## Padding\n",
        "To ensure consistent input shape for modeling, we apply padding to our tokenized text. Padding adjusts the sequence length so that all inputs are of the same length, which is necessary for batch processing in neural networks.\n",
        "\n",
        "In this step, it important to decide on the best maximum length of sequences for padding. This length affects both the model's performance and computational efficiency. If the maximum length is too long, it may lead to increased computational costs and may include lots of padding for shorter sequences. On the other hand, if it's too short, valuable information might be lost.\n",
        "\n",
        "In this section, we will analyze the distribution of sequence lengths in our dataset and choose an appropriate maximum length. This is often a balance between capturing enough information and maintaining computational efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distribution of sequence lengths\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist([len(sequence) for sequence in X_train_sequences], bins=160)\n",
        "plt.show()\n",
        "\n",
        "# print maximum sequence length\n",
        "print(\"Longest sequence: \", max([len(sequence) for sequence in X_train_sequences]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MZS-wM-gN2BH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = None # Set the maxlen parameter to the desired maximum length of your sequences\n",
        "# Note: By setting it to `None`, the maximum length of the sequences will be the length of the longest sequence in the data.\n",
        "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_len)\n",
        "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvdJvsOXN2BJ"
      },
      "source": [
        "## Model Construction\n",
        "Now, we will define our neural network model, incorporating an embedding layer to capture text representation effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = XXX  # Replace with your vocabulary size as defined in the tokenization step\n",
        "max_length = XXX  # Replace with your maximum sequence length as defined in the padding step\n",
        "embedding_dim = 128  # Size of the embedding vectors\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(GlobalAveragePooling1D()),\n",
        "model.add(Dense(20, activation='relu')),\n",
        "model.add(Dense(units=NUMBER_OF_CLASSES, activation='softmax'))\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SyH5KYz1N2BK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of embedding layer weights: (10000, 128)\n"
          ]
        }
      ],
      "source": [
        "# Inspect the embedding layer dimensions after training the model\n",
        "embedding_layer_weights = model.layers[0].get_weights()[0]\n",
        "print(f\"Shape of embedding layer weights: {embedding_layer_weights.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Fitting\n",
        "\n",
        "In this section, we will compile and train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfeAr3j3N2BK"
      },
      "outputs": [],
      "source": [
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='replace_with_suitable_loss_function', metrics=['accuracy']) # The loss function can either be `sparse_categorical_crossentropy` or `categorical_crossentropy`, depending on how you ave encoded your labels\n",
        "history = model.fit(your_train_texts, your_train_labels, epochs=30, batch_size=1024, validation_data=(your_test_texts, your_test_labels)) # Replace your_train_texts, your_train_labels, your_test_texts, your_test_labels with the actual variables containing your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the code of above to compile and train your data again but replace definition of `validation_data` withe the `validation_split` argument (see below). Why is this approach of defining the validation data not recommended?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='replace_with_suitable_loss_function', metrics=['accuracy']) # The loss function can either be `sparse_categorical_crossentropy` or `categorical_crossentropy`, depending on how you ave encoded your labels\n",
        "history_val_split = model.fit(your_train_texts, your_train_labels, epochs=30, batch_size=1024, validation_split=0.2) # Replace your_train_texts and your_train_labels with the actual variables containing your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values for both models of above\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot accuracy for the first model\n",
        "plt.plot(history.history['accuracy'], 'b-', label='Train accuracy Base Model')\n",
        "plt.plot(history.history['val_accuracy'], 'b--', label='Validation accuracy Base Model')\n",
        "\n",
        "# Plot accuracy for the second model\n",
        "plt.plot(history_val_split.history['accuracy'], 'r-', label='Train accuracy Validation Split Model')\n",
        "plt.plot(history_val_split.history['val_accuracy'], 'r--', label='Validation accuracy Validation Split Model')\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Now that you have a basic understanding of how to construct and train a text classification model using Keras, you can experiment with the model to improve its performance. One way to do this is by tuning the model's hyperparameters.\n",
        "\n",
        "Here are a few hyperparameters you can experiment with:\n",
        "\n",
        "1. **Vocabulary Size**: This is the number of unique words in your text data. A larger vocabulary size means the model can recognize more unique words, but it also increases the dimensionality of the data and can lead to overfitting. Try reducing the vocabulary size to see if it improves the model's performance.\n",
        "\n",
        "2. **Maximum Sequence Length**: This is the length of the input sequences. If you increase the maximum sequence length, the model will be able to process longer sequences, but it will also take longer to train and may be more prone to overfitting. Try decreasing the maximum sequence length to see if it improves the model's performance.\n",
        "\n",
        "3. **Embedding Dimensionality**: This is the size of the vectors in which words will be embedded. A higher dimensionality can capture more nuanced relationships between words, but it also increases the computational cost and can lead to overfitting. Try experimenting with different embedding dimensionalities to see what works best.\n",
        "\n",
        "Remember, the goal of hyperparameter tuning is to find the combination of hyperparameters that gives the best performance on your validation data. Happy tuning!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
