{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi9KCIndN2A0"
      },
      "source": [
        "# Sentiment Analysis with Sentiment140 Dataset\n",
        "\n",
        "In this notebook, we aim to accomplish the following:\n",
        "1. Import and analyze the Sentiment140 dataset.\n",
        "2. Prepare the data for model training using tokenization and padding techniques.\n",
        "3. Implement a train-validation split to evaluate our model's performance.\n",
        "4. Train a tokenizer on the training data.\n",
        "5. Construct and train a neural network model that includes an embedding layer for text representation.\n",
        "6. Inspect the output dimensionality of the embedding layer.\n",
        "7. Train the model to classify the sentiment of tweets and evaluate its performance using the validation set.\n",
        "\n",
        "## Introduction\n",
        "For this exercise, we'll be using a subsample of the Sentiment140 dataset from Kaggle.\n",
        "Sentiment140 is a popular dataset for sentiment analysis that contains 1.6 million tweets labeled for sentiment. This dataset is widely used for training machine learning models to differentiate between positive and negative sentiment in text.\n",
        "\n",
        "Here is the link to the dataset: [Sentiment140_small]()\n",
        "\n",
        "Please download the dataset and upload it to your Google Drive.\n",
        "By uploading the data to your Google Drive and not directly to your Colab environment, it is persistently available. You just need to run the cell the code to mount your Google Drive to your Colab environment.\n",
        "\n",
        "The dataset contains the following 6 fields:  \n",
        "`target`: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)  \n",
        "`ids`: The id of the tweet ( 2087)  \n",
        "`date`: the date of the tweet (Sat May 16 23:58:44 UTC 2009)  \n",
        "`flag`: The query (lyx). If there is no query, then this value is NO_QUERY.  \n",
        "`user`: the user that tweeted (robotickilldozr)  \n",
        "`text`: the text of the tweet (Lyx is cool)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmpWkR72N2A-"
      },
      "source": [
        "## Google Drive Setup\n",
        "Before proceeding, ensure that you have uploaded the Sentiment140 dataset to your Google Drive in a specified folder. Then, use the following code to mount your Google Drive and access files using the path '/content/drive/MyDrive/'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jWwiDCON2A_",
        "outputId": "44b0918f-0186-483f-e6df-f9048394c44e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0GBuvowN2BC"
      },
      "source": [
        "## Importing Data\n",
        "Next, we will import the dataset and examine it using descriptive statistics to gain initial insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxHn4UwnN2BD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Update this to the path of your Sentiment140 dataset file\n",
        "file_path = '/content/drive/MyDrive/path_to_your_file.csv'\n",
        "data = pd.read_csv(file_path, encoding='ISO-8859-1')  # Note: The encoding may vary based on your dataset specifics\n",
        "\n",
        "data.head()  # View the first few rows of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descriptive statistics\n",
        "print(data.describe())  # Summary statistics for numerical columns\n",
        "print(data['your_label_column'].value_counts())  # Replace 'your_label_column' with the actual column name for the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing Stop Words\n",
        "\n",
        "Stop words are common words that generally do not contribute much meaning in a sentence and are typically removed in the preprocessing stage of traditional text analysis. This helps in reducing the size of the dataset and improves the performance of the model by focusing on words that carry more meaning.\n",
        "\n",
        "In this section, we will use the NLTK library, a widely-used Python library for natural language processing, to remove stop words from our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stop_words(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [\" \".join([word for word in sentence.split() if word.lower() not in stop_words]) for sentence in data]\n",
        "\n",
        "# Assuming 'data' is your dataframe and it has a column named 'text' containing the tweets\n",
        "data['your_text_column'] = remove_stop_words(data['your_text_column'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQib5xbAN2BD"
      },
      "source": [
        "## Tokenization Function\n",
        "The following function will be responsible for tokenizing our text data. We'll use Keras' Tokenizer class, which allows us to vectorize a text corpus by turning each text into a sequence of integers. The `oov_token` parameter is used to handle out-of-vocabulary words during text conversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PGI4BarN2BE"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "\n",
        "# Implement tokenizer function\n",
        "def tokenizer(sentences, oov_token=\"<OOV>\"):\n",
        "    # Insert your tokenizer code here\n",
        "    return tokenizer\n",
        "\n",
        "# Tokenize the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y65J-BbkN2BG"
      },
      "source": [
        "## Padding\n",
        "To ensure consistent input shape for modeling, we apply padding to our tokenized text. Padding adjusts the sequence length so that all inputs are of the same length, which is necessary for batch processing in neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZS-wM-gN2BH"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "\n",
        "def padding_function(sequences, padding='post', maxlen=None):\n",
        "    # Insert your padding code here\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAzx6uE9N2BI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into train and validation sets\n",
        "train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
        "    data['text'], data['labels'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvdJvsOXN2BJ"
      },
      "source": [
        "## Model Construction\n",
        "Now, we will define and compile our neural network model, incorporating an embedding layer to capture text representation effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEFW7BO2N2BJ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(Dense(units=number_of_classes, activation='softmax'))\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyH5KYz1N2BK"
      },
      "outputs": [],
      "source": [
        "# Inspect the embedding layer dimensions after training the model\n",
        "embedding_layer_weights = model.layers[0].get_weights()[0]\n",
        "print(f\"Shape of embedding layer weights: {embedding_layer_weights.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfeAr3j3N2BK"
      },
      "outputs": [],
      "source": [
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels, epochs=num_epochs, validation_data=(validation_data, validation_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_a1L0SrN2BL"
      },
      "source": [
        "## Conclusion\n",
        "We have covered data preprocessing steps like tokenization and padding, followed by splitting our dataset into training and validation sets. We constructed and trained\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
