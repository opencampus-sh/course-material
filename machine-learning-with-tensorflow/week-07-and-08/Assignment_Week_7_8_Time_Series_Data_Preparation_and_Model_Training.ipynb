{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Week 7 - Time Series Analysis and Neural Network Forecasting\n",
        "\n",
        "In this assignment, you'll work with a complete time series workflow: from data preprocessing to building neural network forecasting models. You'll use the **Air Quality Dataset** from UCI, which contains hourly measurements of air pollutants and weather conditions.\n",
        "\n",
        "## Learning Objectives:\n",
        "- Analyze and preprocess time series data\n",
        "- Engineer relevant features for time series forecasting\n",
        "- Build and compare neural network architectures for forecasting\n",
        "- Evaluate model performance and understand hyperparameter impact\n",
        "\n",
        "**Dataset:** Air Quality Dataset (More than 9,000 hourly observations from March 2004 to February 2005)\n",
        "- More manageable size than Jena Climate dataset\n",
        "- Real-world complexity with missing values and multiple variables\n",
        "- Suitable for learning neural network concepts without excessive training time"
      ],
      "metadata": {
        "id": "cH4kGaAmwj0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Data Download and Initial Analysis\n",
        "\n",
        "Download and explore the Air Quality dataset to understand its structure and characteristics."
      ],
      "metadata": {
        "id": "gNz6G8Zq7LWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5LjtXP2SGW"
      },
      "outputs": [],
      "source": [
        "# Download the Air Quality dataset\n",
        "!wget https://archive.ics.uci.edu/static/public/360/air+quality.zip\n",
        "!unzip -o 'air+quality.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('AirQualityUCI.csv', sep=';', decimal=',')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Nh_9fWKa2fe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Analyze the dataset structure and identify data quality issues\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\nDataset Description:\")\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "TvXw-nwZ5Sj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Examine missing values and data quality\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nUnique values in key columns:\")\n",
        "for col in df.columns[:5]:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "\n",
        "### YOUR CODE: Identify and analyze any anomalies or data quality issues ###"
      ],
      "metadata": {
        "id": "LZ9d4iqyDNiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Data Cleaning and Preprocessing\n",
        "\n",
        "Clean the dataset and prepare it for time series analysis."
      ],
      "metadata": {
        "id": "i4WERJH__nDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create proper datetime index\n",
        "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H.%M.%S')\n",
        "df = df.set_index('DateTime')\n",
        "\n",
        "# 2. Remove unnecessary columns and handle missing values\n",
        "# Remove columns with all NaN or the original Date/Time columns\n",
        "df_clean = df.drop(['Date', 'Time'], axis=1)\n",
        "df_clean = df_clean.dropna(how='all', axis=1)  # Remove columns with all NaN\n",
        "\n",
        "# 3. Handle missing values (marked as -200.0 in this dataset)\n",
        "df_clean = df_clean.replace(-200.0, np.nan)\n",
        "\n",
        "print(\"Cleaned dataset shape:\", df_clean.shape)\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "### YOUR CODE: Apply interpolation to handle remaining missing values ###\n",
        "# Hint: Use appropriate interpolation method for time series data"
      ],
      "metadata": {
        "id": "PsLNKmgjLHFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Select target variable and features\n",
        "# We'll focus on predicting CO(GT) - Carbon Monoxide concentration\n",
        "target_var = 'CO(GT)'\n",
        "feature_cols = ['PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'T', 'RH', 'AH']\n",
        "\n",
        "# Create final dataset\n",
        "df_final = df_clean[[target_var] + feature_cols].copy()\n",
        "df_final = df_final.dropna()  # Remove any remaining NaN values\n",
        "\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "print(f\"Date range: {df_final.index.min()} to {df_final.index.max()}\")\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "CcI6QxcH382e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Exploratory Data Analysis\n",
        "\n",
        "Analyze the time series characteristics of your target variable and key features."
      ],
      "metadata": {
        "id": "Iopu6lncDysB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Plot the target variable over time\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(df_final.index, df_final[target_var])\n",
        "plt.title(f'{target_var} Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('CO Concentration')\n",
        "\n",
        "# 2. Plot seasonal patterns\n",
        "plt.subplot(2, 2, 2)\n",
        "df_final.groupby(df_final.index.month)[target_var].mean().plot(kind='bar')\n",
        "plt.title('Monthly Average CO Levels')\n",
        "plt.xlabel('Month')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "df_final.groupby(df_final.index.hour)[target_var].mean().plot(kind='bar')\n",
        "plt.title('Daily Average CO Levels')\n",
        "plt.xlabel('Hour of Day')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "df_final.groupby(df_final.index.dayofweek)[target_var].mean().plot(kind='bar')\n",
        "plt.title('Weekly Average CO Levels')\n",
        "plt.xlabel('Day of Week (0=Monday)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### YOUR CODE: Analyze and describe the patterns you observe ###"
      ],
      "metadata": {
        "id": "CbuJ42E6ELwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Feature Engineering\n",
        "\n",
        "Create temporal features and apply cyclical encoding where appropriate."
      ],
      "metadata": {
        "id": "6gmQSci1EctG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create basic temporal features\n",
        "df_features = df_final.copy()\n",
        "\n",
        "df_features['Hour'] = df_features.index.hour\n",
        "df_features['DayOfWeek'] = df_features.index.dayofweek\n",
        "df_features['Month'] = df_features.index.month\n",
        "df_features['DayOfYear'] = df_features.index.dayofyear\n",
        "df_features['Weekend'] = (df_features.index.dayofweek >= 5).astype(int)\n",
        "\n",
        "### YOUR CODE: Implement cyclical encoding for temporal features ###\n",
        "# Hint: Use sine/cosine transformations for cyclical features like hour, day of week, etc.\n",
        "# Example: df_features['Hour_sin'] = np.sin(2 * np.pi * df_features['Hour'] / 24)\n",
        "\n",
        "print(\"Features created:\")\n",
        "print(df_features.columns.tolist())"
      ],
      "metadata": {
        "id": "uBmsd0VkRWL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Data Preparation for Neural Networks\n",
        "\n",
        "Prepare the dataset for training neural network models."
      ],
      "metadata": {
        "id": "MAl2n2XYwrEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Normalize the features\n",
        "scaler = StandardScaler()\n",
        "feature_columns = [col for col in df_features.columns if col != target_var]\n",
        "df_scaled = df_features.copy()\n",
        "df_scaled[feature_columns] = scaler.fit_transform(df_features[feature_columns])\n",
        "\n",
        "# First, let's implement the create_sequences function that was missing\n",
        "def create_sequences(data, target_col, sequence_length, forecast_horizon=1):\n",
        "    \"\"\"\n",
        "    Create sequences for time series forecasting\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with time series data\n",
        "        target_col: Name of target column\n",
        "        sequence_length: Length of input sequences\n",
        "        forecast_horizon: How many steps ahead to predict\n",
        "\n",
        "    Returns:\n",
        "        X: Input sequences\n",
        "        y: Target values\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - sequence_length - forecast_horizon + 1):\n",
        "        # Input sequence (all features)\n",
        "        X.append(data.iloc[i:(i + sequence_length)].values)\n",
        "        # Target value (only target column)\n",
        "        y.append(data.iloc[i + sequence_length + forecast_horizon - 1][target_col])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences (make sure df_scaled is defined first)\n",
        "SEQUENCE_LENGTH = 24  # Use 24 hours of data to predict next hour\n",
        "FORECAST_HORIZON = 1  # Predict 1 hour ahead\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(df_scaled, target_var, SEQUENCE_LENGTH, FORECAST_HORIZON)\n",
        "\n",
        "print(f\"Sequences shape: X={X.shape}, y={y.shape}\")"
      ],
      "metadata": {
        "id": "KnJKD3Ilwh6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split data into train/validation/test sets\n",
        "# Use temporal split (no shuffling for time series)\n",
        "\n",
        "# Split data into train/validation/test sets\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.2 * len(X))\n",
        "\n",
        "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
        "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
        "\n",
        "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")"
      ],
      "metadata": {
        "id": "9fW1i8J8R0oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Neural Network Model Implementation\n",
        "\n",
        "Implement and train two different neural network architectures with various hyperparameter combinations."
      ],
      "metadata": {
        "id": "gAF4vjhAwxzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def build_lstm_model(input_shape, lstm_units=50, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build LSTM model for time series forecasting\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(dropout_rate),\n",
        "        LSTM(lstm_units//2, return_sequences=False),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(input_shape, filters=64, kernel_size=3, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build CNN model for time series forecasting\n",
        "    \"\"\"\n",
        "    ### YOUR CODE: Implement CNN architecture ###\n",
        "    # Hint: Use Conv1D layers followed by MaxPooling1D, then Dense layers\n",
        "    model = Sequential([\n",
        "        # Add your CNN layers here\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define hyperparameter combinations to test\n",
        "lstm_configs = [\n",
        "    {'lstm_units': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001},\n",
        "    {'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.001},\n",
        "    {'lstm_units': 50, 'dropout_rate': 0.3, 'learning_rate': 0.0005}\n",
        "]\n",
        "\n",
        "cnn_configs = [\n",
        "    {'filters': 32, 'kernel_size': 3, 'learning_rate': 0.001},\n",
        "    {'filters': 64, 'kernel_size': 5, 'learning_rate': 0.001},\n",
        "    {'filters': 64, 'kernel_size': 3, 'learning_rate': 0.0005}\n",
        "]\n",
        "\n",
        "print(\"Model architectures and hyperparameters defined.\")"
      ],
      "metadata": {
        "id": "zPjsYcGgykpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train LSTM models with different hyperparameters\n",
        "lstm_results = []\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, n_features)\n",
        "\n",
        "for i, config in enumerate(lstm_configs):\n",
        "    print(f\"\\nTraining LSTM Model {i+1} with config: {config}\")\n",
        "\n",
        "    # Build model\n",
        "    model = build_lstm_model(input_shape, **config)\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(patience=5, factor=0.5)\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate model\n",
        "    train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
        "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Store results\n",
        "    result = {\n",
        "        'model_type': 'LSTM',\n",
        "        'config': config,\n",
        "        'train_loss': train_loss[0],\n",
        "        'val_loss': val_loss[0],\n",
        "        'test_loss': test_loss[0],\n",
        "        'train_mae': train_loss[1],\n",
        "        'val_mae': val_loss[1],\n",
        "        'test_mae': test_loss[1],\n",
        "        'training_time': training_time,\n",
        "        'history': history.history\n",
        "    }\n",
        "\n",
        "    lstm_results.append(result)\n",
        "\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    print(f\"Test MSE: {test_loss[0]:.4f}, Test MAE: {test_loss[1]:.4f}\")\n",
        "\n",
        "print(\"\\nAll LSTM models trained successfully!\")"
      ],
      "metadata": {
        "id": "RzqxqfPyt6sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CNN models with different hyperparameters\n",
        "cnn_results = []\n",
        "\n",
        "### YOUR CODE: Implement CNN model training similar to LSTM training above ###\n",
        "# Follow the same pattern as LSTM training but use CNN models\n",
        "\n",
        "print(\"CNN model training completed!\")"
      ],
      "metadata": {
        "id": "f0L74gGfR-u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Model Comparison and Analysis\n",
        "\n",
        "Compare the performance of different models and analyze the impact of hyperparameters."
      ],
      "metadata": {
        "id": "analysis_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model performance\n",
        "all_results = lstm_results + cnn_results\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for result in all_results:\n",
        "    comparison_data.append({\n",
        "        'Model': result['model_type'],\n",
        "        'Config': str(result['config']),\n",
        "        'Test MSE': result['test_loss'],\n",
        "        'Test MAE': result['test_mae'],\n",
        "        'Training Time (s)': result['training_time']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Plot performance comparison\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.bar(range(len(all_results)), [r['test_loss'] for r in all_results])\n",
        "plt.title('Test MSE Comparison')\n",
        "plt.xlabel('Model Index')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(range(len(all_results)), [r['test_mae'] for r in all_results])\n",
        "plt.title('Test MAE Comparison')\n",
        "plt.xlabel('Model Index')\n",
        "plt.ylabel('MAE')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(range(len(all_results)), [r['training_time'] for r in all_results])\n",
        "plt.title('Training Time Comparison')\n",
        "plt.xlabel('Model Index')\n",
        "plt.ylabel('Time (seconds)')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Plot training history for best model\n",
        "best_model = min(all_results, key=lambda x: x['test_loss'])\n",
        "plt.plot(best_model['history']['loss'], label='Training Loss')\n",
        "plt.plot(best_model['history']['val_loss'], label='Validation Loss')\n",
        "plt.title(f'Best Model Training History ({best_model[\"model_type\"]})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### YOUR CODE: Analyze and discuss the results ###\n",
        "# Write your analysis of:\n",
        "# 1. Which model performed best and why?\n",
        "# 2. How did hyperparameters affect performance?\n",
        "# 3. What patterns do you observe in the training curves?\n",
        "# 4. What recommendations would you make for further improvements?"
      ],
      "metadata": {
        "id": "comparison_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8: Reflection and Next Steps\n",
        "\n",
        "**Write your analysis and conclusions here:**\n",
        "\n",
        "### Model Performance Analysis\n",
        "[Analyze which model performed best and why]\n",
        "\n",
        "### Hyperparameter Impact\n",
        "[Discuss how different hyperparameters affected model performance]\n",
        "\n",
        "### Key Insights\n",
        "[What did you learn about time series forecasting with neural networks?]\n",
        "\n",
        "### Recommendations for Improvement\n",
        "[What would you do differently or what additional techniques would you try?]\n",
        "\n",
        "### Optional Extensions (if time permits)\n",
        "- Implement multivariate forecasting using all available features\n",
        "- Try other architectures (GRU, Transformer, etc.)\n",
        "- Implement ensemble methods\n",
        "- Add seasonal decomposition\n",
        "- Experiment with different sequence lengths and forecast horizons"
      ],
      "metadata": {
        "id": "reflection_section"
      }
    }
  ]
}